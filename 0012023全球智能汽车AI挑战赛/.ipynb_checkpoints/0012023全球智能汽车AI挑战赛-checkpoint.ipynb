{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93c4dcf2-088f-4292-a282-6f80a09a926c",
   "metadata": {},
   "source": [
    "# 智能驾驶汽车虚拟仿真视频数据理解赛道Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c183d3-c76a-4cb6-a0d8-75bf8a023a35",
   "metadata": {},
   "source": [
    "赛题链接+数据集链接：https://tianchi.aliyun.com/competition/entrance/532155/introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff408426-2e7f-4418-9c0c-10408aa8b7ef",
   "metadata": {},
   "source": [
    "下面针对本赛事，介绍下解题思路及代码，希望大家都能拿到好成绩。\n",
    "\n",
    "## 解题思路\n",
    "\n",
    "- 基础思路：使用文本与图像进行匹配\n",
    "- 进阶思路：\n",
    "  - 使用图像进行视觉问答\n",
    "  - 时序视频进行视频问答\n",
    "  - 使用多模态大模型进行问答\n",
    "\n",
    "## Clip模型介绍\n",
    "\n",
    "本赛事基于Clip模型实现，下面来介绍一下该模型的一些理论知识。\n",
    "\n",
    "CLIP的全称是Contrastive Language-Image Pre-Training，中文是对比语言-图像预训练，是一个预训练模型，简称为CLIP。\n",
    "\n",
    "该模型是 OpenAI 在 2021 年发布的，最初用于匹配图像和文本的预训练神经网络模型，这个任务在多模态领域比较常见，可以用于文本图像检索，CLIP是近年来在多模态研究领域的经典之作。该模型大量的成对互联网数据进行预训练，在很多任务表现上达到了目前最佳表现（SOTA） 。\n",
    "\n",
    "\n",
    "\n",
    "CLIP的思想非常简单，只需要看懂这幅图就可以了，左边是训练的原理，CLIP一共有两个模态，一个是文本模态，一个是视觉模态，分别对应了Text Encoder和Image Encoder。\n",
    "\n",
    "- Text Encoder用于对文本进行编码，获得其Embedding；\n",
    "- Image Encoder用于对图片编码，获得其Embedding。\n",
    "- 两个Embedding均为一定长度的单一向量。\n",
    "\n",
    "\n",
    "CLIP模型能够实现文本和图像之间的跨模态学习，这意味着它可以理解和关联文本和图像这两种不同的数据类型。通过对文本和图像进行联合学习，CLIP可以更好地理解和生成符合文本描述的图像。\n",
    "\n",
    "由于CLIP模型在预训练阶段已经学习了大量的文本和图像知识，因此它可以在没有见过的新类别上**实现零样本学习**。这意味着CLIP模型可以处理那些在训练时没有见过的新的文本和图像，具有很强的适应能力。\n",
    "\n",
    "\n",
    "\n",
    "## 代码实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0abd816-535d-4ff0-9a0a-3a8baa0f930e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: HF_ENDPOINT=https://hf-mirror.com\n"
     ]
    }
   ],
   "source": [
    "%env HF_ENDPOINT=https://hf-mirror.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a1d20f8-8a07-4653-b28b-6125166338e0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "309a33479f3b4c0cb46104fc8bbb134f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/988 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b53a62da95bc496a818fdf5cdf1a5615",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.71G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9973fdfcbd0d4e3e802886a8127612d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95729e349faa4f3f8c22862532a3c480",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/298 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "238b86eec91d4b4aa026945c610604ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db17d5cc91af4428bcbf284accb23b8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "501ab1897c00491eb3039bb963291381",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a22ce5d535a0497f89892e81842c57a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import glob, json, os\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from tqdm import tqdm_notebook\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e873185b-7008-44fe-9146-2d6857bc6c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "inputs = processor(text=[\"a photo of a cat\", \"a photo of a dog\"], images=image, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "outputs = model(**inputs)\n",
    "logits_per_image = outputs.logits_per_image # this is the image-text similarity score\n",
    "logits_per_image.softmax(dim=1) # we can take the softmax to get the label probabilities\n",
    "\n",
    "cn_match_words = {\n",
    "    \"工况描述\": [\"高速/城市快速路\", \"城区\", \"郊区\", \"隧道\", \"停车场\", \"加油站/充电站\", \"未知\"],\n",
    "    \"天气\": [\"晴天\", \"雨天\", \"多云\", \"雾天\", \"下雪\", \"未知\"],\n",
    "    \"时间\": [\"白天\", \"夜晚\", \"拂晓/日暮\", \"未知\"],\n",
    "    \"道路结构\": [\"十字路口\", \"丁字路口\", \"上下匝道\", \"车道汇入\", \"进出停车场\", \"环岛\", \"正常车道\", \"未知\"],\n",
    "    \"一般障碍物\": [\"雉桶\", \"水马\", \"碎石/石块\", \"井盖\", \"减速带\", \"没有\"],\n",
    "    \"道路异常情况\": [\"油污/水渍\", \"积水\", \"龟裂\", \"起伏不平\", \"没有\", \"未知\"],\n",
    "    \"自车行为\": [\"直行\", \"左转\", \"右转\", \"停止\", \"掉头\", \"加速\", \"减速\", \"变道\", \"其它\"],\n",
    "    \"最近的交通参与者\": [\"行人\", \"小型汽车\", \"卡车\", \"交警\", \"没有\", \"未知\", \"其它\"],\n",
    "    \"最近的交通参与者行为\": [\"直行\", \"左转\", \"右转\", \"停止\", \"掉头\", \"加速\", \"减速\", \"变道\", \"其它\"],\n",
    "}\n",
    "\n",
    "en_match_words = {\n",
    "    \"scerario\": [\"cityroad\", \"urban area\", \"suburb\", \"tunnel\", \"parking lot\", \"gas station/charging station\"],\n",
    "    \"weather\": [\"sunny\", \"rainy\", \"cloudy\", \"foggy\", \"snowy\"],\n",
    "    \"period\": [\"daytime\", \"night\", \"dawn/dusk\"],\n",
    "    \"road_structure\": [\"intersection\", \"T-junction\", \"on-ramp/off-ramp\", \"merge lane\", \"enter/exit parking lot\", \"roundabout\", \"regular lane\", \"unknown\"],\n",
    "    \"general_obstacle\": [\"speed bump\", \"water horse\", \"gravel/stones\", \"manhole cover\", \"speed bump\", \"none\"],\n",
    "    \"abnormal_condition\": [\"oil stains/water stains\", \"water accumulation\", \"cracks\", \"uneven surface\", \"none\", \"unknown\"],\n",
    "    \"ego_car_behavior\": [\"straight\", \"turning left\", \"turning right\", \"stop\", \"U-turn\", \"accelerate\", \"decelerate\", \"change lanes\", \"other\"],\n",
    "    \"closest_participants_type\": [\"pedestrian\", \"small car\", \"truck\", \"traffic police\", \"none\", \"unknown\", \"other\"], \n",
    "    \"closest_participants_behavior\": [\"go straight\", \"turn left\", \"turn right\", \"stop\", \"make a U-turn\", \"accelerate\", \"decelerate\", \"change lanes\", \"other\"]\n",
    "}\n",
    "\n",
    "cap = cv2.VideoCapture('./初赛测试视频/41.avi')\n",
    "img = cap.read()[1]\n",
    "image = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "image = Image.fromarray(image)\n",
    "\n",
    "image.resize((600, 300))\n",
    "\n",
    "\n",
    "submit_json = {\n",
    "    \"作者\" : \"阿水\" ,\n",
    "    \"时间\" : \"231011\",\n",
    "    \"模型名字\" : \"model_name\",\n",
    "    \"测试结果\" : []\n",
    "}\n",
    "\n",
    "submit_json = {\n",
    "    \"author\" : \"abc\" ,\n",
    "    \"time\" : \"231011\",\n",
    "    \"model\" : \"model_name\",\n",
    "    \"test_results\" : []\n",
    "}\n",
    "\n",
    "paths = glob.glob('./初赛测试视频/*')\n",
    "paths.sort()\n",
    "\n",
    "for video_path in paths:\n",
    "    print(video_path)\n",
    "\n",
    "    clip_id = video_path.split('/')[-1]\n",
    "    # clip_id = video_path.split('/')[-1][:-4]\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    img = cap.read()[1]\n",
    "    image = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    image = Image.fromarray(image)\n",
    "\n",
    "    single_video_result = {\n",
    "        \"clip_id\": clip_id,\n",
    "        \"scerario\" : \"cityroad\",\n",
    "        \"weather\":\"unknown\",\n",
    "        \"period\":\"night\",\n",
    "        \"road_structure\":\"ramp\",\n",
    "        \"general_obstacle\":\"nothing\",\n",
    "        \"abnormal_condition\":\"nothing\",\n",
    "        \"ego_car_behavior\":\"turning right\",\n",
    "        \"closest_participants_type\":\"passenger car\",\n",
    "        \"closest_participants_behavior\":\"braking\"\n",
    "    }\n",
    "\n",
    "    for keyword in en_match_words.keys():\n",
    "        if keyword not in [\"weather\", \"road_structure\", 'scerario']:\n",
    "            continue\n",
    "\n",
    "        texts = np.array(en_match_words[keyword])\n",
    "        inputs = processor(text=list(texts), images=image, return_tensors=\"pt\", padding=True)\n",
    "        outputs = model(**inputs)\n",
    "        logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n",
    "        probs = logits_per_image.softmax(dim=1)  # probs: [[1.2686e-03, 5.4499e-02, 6.7968e-04, 9.4355e-01]]\n",
    "\n",
    "        single_video_result[keyword] = texts[probs[0].argsort().numpy()[::-1][0]]\n",
    "\n",
    "    submit_json[\"test_results\"].append(single_video_result)\n",
    "\n",
    "len(paths)\n",
    "\n",
    "with open('coggle_result.json', 'w', encoding='utf-8') as up:\n",
    "    json.dump(submit_json, up, ensure_ascii=False)\n",
    "\n",
    "# \"作者\" : \"abc\" ,\n",
    "# \"时间\" : \"YYMMDD\",\n",
    "# \"模型名字\" : \"model_name\",\n",
    "# \"测试结果\" :[\n",
    "# {\n",
    "# \"视频ID\" : \"xxxx_1\",\n",
    "# \"工况描述\" : \"城市道路\",\n",
    "# \"天气\":\"未知\",\n",
    "# \"时间\":\"夜晚\",\n",
    "# \"道路结构\":\"匝道\",\n",
    "# \"一般障碍物\":\"无\",\n",
    "# \"道路异常情况\":\"无\",\n",
    "# \"自车行为\":\"右转\",\n",
    "# \"最近的交通参与者\":\"小轿车\",\n",
    "# \"最近的交通参与者行为\":\"制动\"\n",
    "# },\n",
    "\n",
    "submit_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e90e647-9ec5-4630-8339-777fea3069eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "参考资料；https://blog.csdn.net/manongtuzi/article/details/135589689\n",
    "这个链接给出了很多详细的实现"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kewei-ai",
   "language": "python",
   "name": "kewei-ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
